#!/usr/bin/env python3
"""
Explorador de Datos F1 - AnÃ¡lisis completo del dataset
Analiza todas las caracterÃ­sticas disponibles, distribuciones y estadÃ­sticas
"""

import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class F1DataExplorer:
    def __init__(self):
        self.cache_dir = "app/models_cache"
        self.raw_data_dir = os.path.join(self.cache_dir, "raw_data")
        self.data = None
        self.analysis_results = {}
        
    def load_all_data(self):
        """Carga todos los datos disponibles"""
        print("ğŸ” CARGANDO TODOS LOS DATOS DISPONIBLES")
        print("=" * 60)
        
        all_data = []
        files_loaded = 0
        
        if not os.path.exists(self.raw_data_dir):
            print(f"âŒ No se encuentra el directorio: {self.raw_data_dir}")
            return None
            
        # Buscar todos los archivos .pkl en raw_data
        for filename in os.listdir(self.raw_data_dir):
            if filename.endswith('.pkl') and 'complete' in filename:
                file_path = os.path.join(self.raw_data_dir, filename)
                try:
                    print(f"ğŸ“¦ Cargando: {filename}")
                    with open(file_path, 'rb') as f:
                        race_data = pickle.load(f)
                        if isinstance(race_data, dict):
                            # Intentar diferentes claves posibles
                            if 'data' in race_data:
                                race_df = race_data['data']
                            elif 'race_data' in race_data:
                                race_df = race_data['race_data']
                            else:
                                race_df = None
                                
                            if race_df is not None and not race_df.empty:
                                all_data.append(race_df)
                                files_loaded += 1
                        elif isinstance(race_data, pd.DataFrame):
                            if not race_data.empty:
                                all_data.append(race_data)
                                files_loaded += 1
                except Exception as e:
                    print(f"   âš ï¸  Error cargando {filename}: {e}")
        
        if all_data:
            print(f"\nâœ… {files_loaded} archivos cargados exitosamente")
            self.data = pd.concat(all_data, ignore_index=True)
            print(f"ğŸ“Š Dataset final: {len(self.data)} filas, {len(self.data.columns)} columnas")
            return self.data
        else:
            print("âŒ No se encontraron datos vÃ¡lidos")
            return None
    
    def general_overview(self):
        """AnÃ¡lisis general del dataset"""
        if self.data is None:
            print("âŒ No hay datos cargados")
            return
            
        print("\nğŸ” RESUMEN GENERAL DEL DATASET")
        print("=" * 60)
        
        # InformaciÃ³n bÃ¡sica
        print(f"ğŸ“Š Dimensiones: {self.data.shape[0]:,} filas Ã— {self.data.shape[1]} columnas")
        print(f"ğŸ’¾ TamaÃ±o en memoria: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # InformaciÃ³n temporal
        if 'year' in self.data.columns:
            years = sorted(self.data['year'].unique())
            print(f"ğŸ“… AÃ±os disponibles: {years[0]} - {years[-1]} ({len(years)} aÃ±os)")
            
            # DistribuciÃ³n por aÃ±o
            year_counts = self.data['year'].value_counts().sort_index()
            print(f"\nğŸ“ˆ DistribuciÃ³n por aÃ±o:")
            for year, count in year_counts.items():
                percentage = (count / len(self.data)) * 100
                print(f"   {year}: {count:,} registros ({percentage:.1f}%)")
        
        # InformaciÃ³n de carreras
        if 'race_name' in self.data.columns:
            races = self.data['race_name'].nunique()
            print(f"ğŸ Carreras Ãºnicas: {races}")
            
        # InformaciÃ³n de pilotos
        if 'driver' in self.data.columns:
            drivers = self.data['driver'].nunique()
            print(f"ğŸï¸  Pilotos Ãºnicos: {drivers}")
            
        # Valores faltantes
        missing_data = self.data.isnull().sum()
        missing_percentage = (missing_data / len(self.data)) * 100
        columns_with_missing = missing_data[missing_data > 0].sort_values(ascending=False)
        
        if len(columns_with_missing) > 0:
            print(f"\nâš ï¸  Columnas con valores faltantes:")
            for col, missing_count in columns_with_missing.head(10).items():
                pct = missing_percentage[col]
                print(f"   {col}: {missing_count:,} ({pct:.1f}%)")
        else:
            print(f"\nâœ… Sin valores faltantes")
            
        self.analysis_results['general'] = {
            'shape': self.data.shape,
            'memory_mb': self.data.memory_usage(deep=True).sum() / 1024**2,
            'years': sorted(self.data['year'].unique()) if 'year' in self.data.columns else [],
            'missing_data': columns_with_missing.to_dict()
        }
    
    def analyze_columns_by_type(self):
        """Analiza columnas por tipo de dato"""
        if self.data is None:
            return
            
        print("\nğŸ” ANÃLISIS DE COLUMNAS POR TIPO")
        print("=" * 60)
        
        # Clasificar columnas por tipo
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = self.data.select_dtypes(include=['object']).columns.tolist()
        datetime_cols = self.data.select_dtypes(include=['datetime64']).columns.tolist()
        boolean_cols = self.data.select_dtypes(include=['bool']).columns.tolist()
        
        print(f"ğŸ”¢ Columnas numÃ©ricas ({len(numeric_cols)}):")
        for col in numeric_cols:
            dtype_info = str(self.data[col].dtype)
            unique_vals = self.data[col].nunique()
            print(f"   {col} ({dtype_info}) - {unique_vals:,} valores Ãºnicos")
            
        print(f"\nğŸ“ Columnas categÃ³ricas ({len(categorical_cols)}):")
        for col in categorical_cols:
            unique_vals = self.data[col].nunique()
            print(f"   {col} - {unique_vals:,} valores Ãºnicos")
            
        if datetime_cols:
            print(f"\nğŸ“… Columnas de fecha/hora ({len(datetime_cols)}):")
            for col in datetime_cols:
                print(f"   {col}")
                
        if boolean_cols:
            print(f"\nâœ… Columnas booleanas ({len(boolean_cols)}):")
            for col in boolean_cols:
                true_count = self.data[col].sum()
                total = len(self.data)
                print(f"   {col} - {true_count:,}/{total:,} verdaderos ({true_count/total*100:.1f}%)")
        
        self.analysis_results['column_types'] = {
            'numeric': numeric_cols,
            'categorical': categorical_cols,
            'datetime': datetime_cols,
            'boolean': boolean_cols
        }
    
    def analyze_weather_data(self):
        """AnÃ¡lisis especÃ­fico de datos meteorolÃ³gicos"""
        print("\nğŸŒ¤ï¸  ANÃLISIS DE DATOS METEOROLÃ“GICOS")
        print("=" * 60)
        
        weather_cols = [col for col in self.data.columns if any(w in col.lower() 
                       for w in ['weather', 'temp', 'humidity', 'rain', 'wind', 'pressure', 'air_temp', 'track_temp'])]
        
        if not weather_cols:
            print("âŒ No se encontraron columnas meteorolÃ³gicas")
            return
            
        print(f"ğŸŒ¡ï¸  Columnas meteorolÃ³gicas encontradas ({len(weather_cols)}):")
        for col in weather_cols:
            if col in self.data.columns:
                if self.data[col].dtype in ['object']:
                    unique_vals = self.data[col].value_counts()
                    print(f"   {col}:")
                    for val, count in unique_vals.head(5).items():
                        print(f"      {val}: {count:,} registros")
                else:
                    stats = self.data[col].describe()
                    if len(stats) >= 4:
                        print(f"   {col}: min={self.data[col].min():.2f}, max={self.data[col].max():.2f}, media={self.data[col].mean():.2f}")
        
        # AnÃ¡lisis de lluvia si existe
        rain_cols = [col for col in self.data.columns if 'rain' in col.lower()]
        if rain_cols:
            print(f"\nğŸŒ§ï¸  AnÃ¡lisis de lluvia:")
            for col in rain_cols:
                if col in self.data.columns:
                    rain_distribution = self.data[col].value_counts()
                    print(f"   {col}:")
                    for val, count in rain_distribution.items():
                        pct = count / len(self.data) * 100
                        print(f"      {val}: {count:,} ({pct:.1f}%)")
    
    def analyze_performance_metrics(self):
        """AnÃ¡lisis de mÃ©tricas de rendimiento"""
        print("\nğŸ ANÃLISIS DE MÃ‰TRICAS DE RENDIMIENTO")
        print("=" * 60)
        
        performance_cols = [col for col in self.data.columns if any(p in col.lower() 
                           for p in ['lap_time', 'position', 'points', 'speed', 'sector', 'stint', 'tire'])]
        
        if not performance_cols:
            print("âŒ No se encontraron columnas de rendimiento")
            return
            
        print(f"ğŸï¸  Columnas de rendimiento encontradas ({len(performance_cols)}):")
        
        for col in performance_cols:
            if col in self.data.columns:
                if self.data[col].dtype in ['object']:
                    unique_vals = self.data[col].nunique()
                    print(f"   {col}: {unique_vals} valores Ãºnicos")
                    if unique_vals <= 10:
                        val_counts = self.data[col].value_counts()
                        print(f"      DistribuciÃ³n: {dict(val_counts.head(5))}")
                else:
                    if self.data[col].dtype in [np.number, 'float64', 'int64']:
                        print(f"   {col}: min={self.data[col].min():.3f}, max={self.data[col].max():.3f}, media={self.data[col].mean():.3f}")
    
    def analyze_by_year(self):
        """AnÃ¡lisis detallado por aÃ±o"""
        if 'year' not in self.data.columns:
            print("\nâŒ No se encontrÃ³ columna 'year' para anÃ¡lisis temporal")
            return
            
        print("\nğŸ“… ANÃLISIS DETALLADO POR AÃ‘O")
        print("=" * 60)
        
        years = sorted(self.data['year'].unique())
        
        for year in years:
            year_data = self.data[self.data['year'] == year]
            print(f"\nğŸ† AÃ‘O {year}")
            print("-" * 40)
            
            # EstadÃ­sticas bÃ¡sicas
            print(f"ğŸ“Š Registros: {len(year_data):,}")
            
            # Carreras en ese aÃ±o
            if 'race_name' in self.data.columns:
                races = year_data['race_name'].nunique()
                print(f"ğŸ Carreras: {races}")
                
            # Pilotos en ese aÃ±o
            if 'driver' in self.data.columns:
                drivers = year_data['driver'].nunique()
                print(f"ğŸï¸  Pilotos: {drivers}")
                
            # AnÃ¡lisis meteorolÃ³gico del aÃ±o
            weather_cols = [col for col in year_data.columns if any(w in col.lower() 
                           for w in ['air_temp', 'track_temp', 'humidity'])]
            
            if weather_cols:
                print(f"ğŸŒ¤ï¸  Condiciones meteorolÃ³gicas promedio:")
                for col in weather_cols:
                    if col in year_data.columns and year_data[col].dtype in [np.number]:
                        avg_val = year_data[col].mean()
                        print(f"   {col}: {avg_val:.2f}")
                        
            # AnÃ¡lisis de lluvia
            rain_cols = [col for col in year_data.columns if 'rain' in col.lower()]
            if rain_cols:
                for col in rain_cols:
                    if col in year_data.columns:
                        rain_percentage = (year_data[col] == 'SÃ­').sum() / len(year_data) * 100 if 'SÃ­' in year_data[col].values else 0
                        print(f"ğŸŒ§ï¸  Porcentaje con lluvia: {rain_percentage:.1f}%")
                        break
                        
            # Performance metrics del aÃ±o
            if 'lap_time_seconds' in year_data.columns:
                avg_lap_time = year_data['lap_time_seconds'].mean()
                print(f"â±ï¸  Tiempo de vuelta promedio: {avg_lap_time:.3f}s")
                
            if 'points' in year_data.columns:
                total_points = year_data['points'].sum()
                print(f"ğŸ† Total puntos otorgados: {total_points}")
    
    def identify_features_for_training(self):
        """Identifica las mejores caracterÃ­sticas para entrenar modelos"""
        print("\nğŸ¤– CARACTERÃSTICAS RECOMENDADAS PARA ENTRENAMIENTO")
        print("=" * 60)
        
        # CaracterÃ­sticas numÃ©ricas Ãºtiles
        numeric_features = []
        categorical_features = []
        target_candidates = []
        
        for col in self.data.columns:
            col_lower = col.lower()
            
            # Candidatos para variables objetivo
            if any(target in col_lower for target in ['position', 'points', 'lap_time']):
                if self.data[col].dtype in [np.number]:
                    target_candidates.append(col)
                    
            # CaracterÃ­sticas numÃ©ricas Ãºtiles
            elif any(feature in col_lower for feature in [
                'temp', 'humidity', 'pressure', 'speed', 'sector', 'stint',
                'tire', 'fuel', 'drs', 'gap', 'interval'
            ]):
                if self.data[col].dtype in [np.number]:
                    numeric_features.append(col)
                    
            # CaracterÃ­sticas categÃ³ricas Ãºtiles  
            elif any(feature in col_lower for feature in [
                'driver', 'team', 'compound', 'weather', 'rain', 'track'
            ]):
                if self.data[col].dtype == 'object':
                    categorical_features.append(col)
        
        print(f"ğŸ¯ Variables objetivo candidatas ({len(target_candidates)}):")
        for target in target_candidates:
            unique_vals = self.data[target].nunique()
            print(f"   {target} - {unique_vals:,} valores Ãºnicos")
            
        print(f"\nğŸ”¢ CaracterÃ­sticas numÃ©ricas Ãºtiles ({len(numeric_features)}):")
        for feature in numeric_features:
            missing_pct = (self.data[feature].isnull().sum() / len(self.data)) * 100
            print(f"   {feature} - {missing_pct:.1f}% faltantes")
            
        print(f"\nğŸ“ CaracterÃ­sticas categÃ³ricas Ãºtiles ({len(categorical_features)}):")
        for feature in categorical_features:
            unique_vals = self.data[feature].nunique()
            missing_pct = (self.data[feature].isnull().sum() / len(self.data)) * 100
            print(f"   {feature} - {unique_vals} categorÃ­as, {missing_pct:.1f}% faltantes")
        
        # Recomendaciones
        print(f"\nğŸ’¡ RECOMENDACIONES:")
        print(f"âœ… Para predicciÃ³n de posiciÃ³n: usar 'position' como objetivo")
        print(f"âœ… Para predicciÃ³n de puntos: usar 'points' como objetivo")
        print(f"âœ… Para predicciÃ³n de tiempo: usar variables de 'lap_time'")
        print(f"âš ï¸  Aplicar encoding a variables categÃ³ricas antes del entrenamiento")
        print(f"âš ï¸  Normalizar variables numÃ©ricas con diferentes escalas")
        print(f"âš ï¸  Considerar crear features de interacciÃ³n (ej: temp * humidity)")
        
        self.analysis_results['features'] = {
            'target_candidates': target_candidates,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features
        }
    
    def generate_data_quality_report(self):
        """Genera un reporte de calidad de datos"""
        print("\nğŸ“‹ REPORTE DE CALIDAD DE DATOS")
        print("=" * 60)
        
        total_rows = len(self.data)
        
        # Completitud de datos
        completeness = {}
        for col in self.data.columns:
            non_null_count = self.data[col].count()
            completeness[col] = (non_null_count / total_rows) * 100
            
        # Columnas con alta completitud (>95%)
        high_quality_cols = [col for col, comp in completeness.items() if comp >= 95]
        print(f"âœ… Columnas con alta calidad (â‰¥95% completas): {len(high_quality_cols)}")
        
        # Columnas con baja completitud (<50%)
        low_quality_cols = [col for col, comp in completeness.items() if comp < 50]
        if low_quality_cols:
            print(f"âŒ Columnas con baja calidad (<50% completas): {len(low_quality_cols)}")
            for col in low_quality_cols:
                print(f"   {col}: {completeness[col]:.1f}% completa")
        else:
            print(f"âœ… No hay columnas con baja calidad")
            
        # Duplicados
        duplicates = self.data.duplicated().sum()
        print(f"\nğŸ”„ Filas duplicadas: {duplicates:,} ({duplicates/total_rows*100:.2f}%)")
        
        # Consistency checks
        print(f"\nğŸ” Verificaciones de consistencia:")
        
        # Verificar aÃ±os vÃ¡lidos
        if 'year' in self.data.columns:
            invalid_years = self.data[(self.data['year'] < 2000) | (self.data['year'] > 2030)]
            print(f"   AÃ±os invÃ¡lidos: {len(invalid_years)} registros")
            
        # Verificar posiciones vÃ¡lidas
        if 'position' in self.data.columns:
            invalid_positions = self.data[(self.data['position'] < 1) | (self.data['position'] > 25)]
            print(f"   Posiciones invÃ¡lidas: {len(invalid_positions)} registros")
    
    def save_analysis_results(self):
        """Guarda los resultados del anÃ¡lisis"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Crear directorio si no existe
        analysis_dir = os.path.join(self.cache_dir, "data_analysis")
        os.makedirs(analysis_dir, exist_ok=True)
        
        # Guardar resultados
        results_file = os.path.join(analysis_dir, f"data_exploration_{timestamp}.pkl")
        with open(results_file, 'wb') as f:
            pickle.dump(self.analysis_results, f)
        
        # Generar reporte de texto
        report_file = os.path.join(analysis_dir, f"data_exploration_report_{timestamp}.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ“Š REPORTE DE EXPLORACIÃ“N DE DATOS F1\n")
            f.write(f"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 60 + "\n\n")
            
            if 'general' in self.analysis_results:
                f.write(f"RESUMEN GENERAL:\n")
                f.write(f"Dimensiones: {self.analysis_results['general']['shape']}\n")
                f.write(f"Memoria: {self.analysis_results['general']['memory_mb']:.2f} MB\n")
                f.write(f"AÃ±os: {self.analysis_results['general']['years']}\n\n")
                
            if 'features' in self.analysis_results:
                f.write(f"CARACTERÃSTICAS PARA ENTRENAMIENTO:\n")
                f.write(f"Variables objetivo: {self.analysis_results['features']['target_candidates']}\n")
                f.write(f"Features numÃ©ricas: {len(self.analysis_results['features']['numeric_features'])}\n")
                f.write(f"Features categÃ³ricas: {len(self.analysis_results['features']['categorical_features'])}\n")
        
        print(f"\nğŸ’¾ Resultados guardados en:")
        print(f"   ğŸ“Š Datos: {results_file}")
        print(f"   ğŸ“„ Reporte: {report_file}")
    
    def run_complete_analysis(self):
        """Ejecuta el anÃ¡lisis completo"""
        print("ğŸï¸  F1 DATA EXPLORER")
        print("ğŸ” Iniciando anÃ¡lisis completo del dataset...")
        print("=" * 60)
        
        # Cargar datos
        if self.load_all_data() is None:
            return
            
        # Ejecutar todos los anÃ¡lisis
        self.general_overview()
        self.analyze_columns_by_type()
        self.analyze_weather_data()
        self.analyze_performance_metrics()
        self.analyze_by_year()
        self.identify_features_for_training()
        self.generate_data_quality_report()
        
        # Guardar resultados
        self.save_analysis_results()
        
        print(f"\nğŸ‰ Â¡ANÃLISIS COMPLETADO!")
        print(f"ğŸ“Š Dataset analizado: {len(self.data):,} registros")
        print(f"ğŸ“‹ Columnas analizadas: {len(self.data.columns)}")
        print(f"ğŸ“ˆ AnÃ¡lisis guardado en: app/models_cache/data_analysis/")

def main():
    explorer = F1DataExplorer()
    explorer.run_complete_analysis()

if __name__ == "__main__":
    main()
