import pickle
import os
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score, GridSearchCV, TimeSeriesSplit, KFold
import numpy as np
from app.core.predictors.random_forest import RandomForestPredictor
from app.core.predictors.xgboost_model import XGBoostPredictor
from app.core.predictors.gradient_boosting import GradientBoostingPredictor

class ModelTrainer:
    def __init__(self, use_time_series_cv=True):
        self.use_time_series_cv = use_time_series_cv
        self.models = self._initialize_models_with_hyperparams()
        self.results = {}

    def _initialize_models_with_hyperparams(self):
        return {
            'RandomForest': {
                'model_class': RandomForestPredictor,
                'param_grid': {
                    'n_estimators': [50, 100],
                    'max_depth': [3, 5],
                    'min_samples_split': [5, 10],
                    'min_samples_leaf': [2, 4]
                }
            },
            'XGBoost': {
                'model_class': XGBoostPredictor,
                'param_grid': {
                    'n_estimators': [30, 50, 100],
                    'max_depth': [3, 5],
                    'learning_rate': [0.01, 0.05, 0.1],
                    'subsample': [0.6, 0.8],
                    'colsample_bytree': [0.6, 0.8],
                    'reg_alpha': [0.01, 0.1],  # L1 regularization
                    'reg_lambda': [0.01, 0.1]  # L2 regularization
                }
            },
            'GradientBoosting': {
                'model_class': GradientBoostingPredictor,
                'param_grid': {
                    'n_estimators': [30, 50],  # REDUCIR para evitar overfitting
                    'max_depth': [2, 3],       # REDUCIR profundidad
                    'learning_rate': [0.01, 0.05],  # LEARNING RATE m√°s bajo
                    'subsample': [0.6, 0.8],
                    'min_samples_split': [10, 20],  # AUMENTAR splits m√≠nimos
                    'min_samples_leaf': [5, 10]     # AUMENTAR hojas m√≠nimas
                }
            }
        }

    def train_all_models(self, X_train, X_test, y_train, y_test, label_encoder, feature_names):
        print("\U0001F682 Entrenando modelos con Cross-Validation...")

        if X_train is None or y_train is None:
            print("‚ùå No hay datos para entrenar")
            return {}

        if len(X_train) < 30:
            print("üõë Dataset chico: reduciendo hiperpar√°metros para evitar overfitting")
            self.models['RandomForest']['param_grid']['max_depth'] = [3]
            self.models['XGBoost']['param_grid']['max_depth'] = [3]
            self.models['GradientBoosting']['param_grid']['max_depth'] = [3]

        self._save_metadata(label_encoder, feature_names)
        cv_splitter = self._get_cv_splitter(X_train)

        for name, model_config in self.models.items():
            print(f"\n{'='*50}")
            print(f"üîç ENTRENANDO {name} CON CROSS-VALIDATION")
            print(f"{'='*50}")

            self._train_single_model_with_cv(
                name, model_config, X_train, X_test, y_train, y_test, cv_splitter
            )

        self._show_detailed_comparison()

        self._save_training_results(self.results)
        
        return self.results

    def _get_cv_splitter(self, X_train):
        n_samples = len(X_train)
        if self.use_time_series_cv and n_samples > 10:
            n_splits = min(5, n_samples // 5)
            print(f"üìä Usando TimeSeriesSplit con {n_splits} splits")
            return TimeSeriesSplit(n_splits=n_splits)
        else:
            n_splits = min(3, n_samples // 5)
            print(f"üìä Usando KFold con {n_splits} splits")
            return KFold(n_splits=n_splits, shuffle=True, random_state=42)

    def _train_single_model_with_cv(self, name, model_config, X_train, X_test, y_train, y_test, cv_splitter):
        try:
            model_class = model_config['model_class']
            base_model = model_class()

            print(f"üîß Optimizando hiperpar√°metros para {name}...")

            fit_params = {}
            if name == 'XGBoost':
                # Para XGBoost usar eval_set sin early_stopping en GridSearchCV
                fit_params = {}  # Vac√≠o para GridSearchCV
            elif hasattr(base_model.model, "early_stopping_rounds"):
                fit_params = {
                    "eval_set": [(X_test, y_test)],
                    "early_stopping_rounds": 10,
                    "verbose": False
                }

            if len(X_train) > 20:
                # CONFIGURACI√ìN ESPECIAL PARA XGBOOST
                if name == 'XGBoost':
                    grid_search = GridSearchCV(
                        base_model.model,
                        model_config['param_grid'],
                        cv=cv_splitter,
                        scoring='neg_mean_squared_error',
                        n_jobs=-1,
                        verbose=0
                        )
                else:
                    grid_search = GridSearchCV(
                        base_model.model,
                        model_config['param_grid'],
                        cv=cv_splitter,
                        scoring='neg_mean_squared_error',
                        n_jobs=-1,
                        verbose=0
                    )

                print(f"üîç DEBUG: param_grid = {model_config['param_grid']}")
                combinations = self._get_param_combinations(model_config['param_grid'])
                print(f"üîç Calculando {combinations} combinaciones de hiperpar√°metros...")
                grid_search.fit(X_train, y_train)  # Sin fit_params para XGBoost
                
                # üìä MOSTRAR DETALLES DE LA B√öSQUEDA DE HIPERPAR√ÅMETROS
                self._show_hyperparameter_search_details(grid_search, name)
                
                best_model = grid_search.best_estimator_
                best_params = grid_search.best_params_

                print(f"‚úÖ Mejores par√°metros: {best_params}")
                print(f"‚úÖ Mejor CV score: {-grid_search.best_score_:.4f}")
            else:
                print(f"‚ö†Ô∏è  Dataset peque√±o ({len(X_train)} muestras), usando par√°metros por defecto")
                base_model.train(X_train, y_train)
                best_model = base_model.model
                best_params = "default"

            print(f"üìä Evaluando con Cross-Validation...")
            cv_scores = cross_val_score(
                best_model, X_train, y_train, 
                cv=cv_splitter, 
                scoring='neg_mean_squared_error'
            )

            cv_mse_scores = -cv_scores
            cv_mean = cv_mse_scores.mean()
            cv_std = cv_mse_scores.std()

            y_pred_train = best_model.predict(X_train)
            y_pred_test = best_model.predict(X_test)

            metrics = {
                'cv_mse_mean': cv_mean,
                'cv_mse_std': cv_std,
                'cv_scores': cv_mse_scores.tolist(),
                'train_mse': mean_squared_error(y_train, y_pred_train),
                'train_mae': mean_absolute_error(y_train, y_pred_train),
                'train_r2': r2_score(y_train, y_pred_train),
                'test_mse': mean_squared_error(y_test, y_pred_test),
                'test_mae': mean_absolute_error(y_test, y_pred_test),
                'test_r2': r2_score(y_test, y_pred_test),
                'best_params': best_params,
                'overfitting_score': self._calculate_overfitting_score(
                    mean_squared_error(y_train, y_pred_train),
                    mean_squared_error(y_test, y_pred_test)
                )
            }

            if metrics['train_mse'] < 0.0001 and metrics['test_mse'] > 0.05:
                print("üö® El modelo est√° memorizando los datos de entrenamiento")

            self.results[name] = metrics
            self._show_model_results(name, metrics)

            optimized_model = model_class()
            optimized_model.model = best_model
            self._save_model(name, optimized_model)

        except Exception as e:
            print(f"‚ùå Error entrenando {name}: {e}")
            self.results[name] = {'error': str(e)}

    def _get_param_combinations(self, param_grid):
        """Calcula el n√∫mero total de combinaciones de hiperpar√°metros"""
        total = 1
        for param_values in param_grid.values():
            if isinstance(param_values, (list, tuple)):
                total *= len(param_values)
            else:
                total *= 1  # Si es un valor √∫nico
        return total

    def _show_hyperparameter_search_details(self, grid_search, model_name):
        """Muestra detalles detallados de la b√∫squeda de hiperpar√°metros"""
        print(f"\nüî¨ AN√ÅLISIS DETALLADO DE HIPERPAR√ÅMETROS - {model_name}")
        print("-" * 60)
        
        # Obtener todos los resultados
        results_df = pd.DataFrame(grid_search.cv_results_)
        
        # Mostrar top 5 mejores combinaciones
        top_results = results_df.nlargest(5, 'mean_test_score')
        
        print(f"üèÜ TOP 5 MEJORES COMBINACIONES:")
        for i, (idx, row) in enumerate(top_results.iterrows(), 1):
            score = -row['mean_test_score']  # Convertir de negativo
            std = row['std_test_score']
            params = row['params']
            
            print(f"\n   {i}. Score: {score:.4f} (¬±{std:.4f})")
            print(f"      Par√°metros: {params}")
        
        # Mostrar estad√≠sticas generales
        all_scores = -results_df['mean_test_score']
        print(f"\nüìä ESTAD√çSTICAS DE LA B√öSQUEDA:")
        print(f"   üéØ Mejor score: {all_scores.min():.4f}")
        print(f"   üìà Score promedio: {all_scores.mean():.4f}")
        print(f"   üìâ Peor score: {all_scores.max():.4f}")
        print(f"   üìè Desviaci√≥n est√°ndar: {all_scores.std():.4f}")
        print(f"   üîÑ Mejora vs promedio: {((all_scores.mean() - all_scores.min()) / all_scores.mean() * 100):.1f}%")
        
        # Analizar importancia de par√°metros
        self._analyze_parameter_importance(results_df, model_name)

    def _analyze_parameter_importance(self, results_df, model_name):
        """Analiza qu√© par√°metros tienen m√°s impacto en el rendimiento"""
        print(f"\nüîç AN√ÅLISIS DE IMPORTANCIA DE PAR√ÅMETROS:")
        
        # Convertir params a columnas separadas
        param_columns = {}
        for idx, row in results_df.iterrows():
            for param, value in row['params'].items():
                if param not in param_columns:
                    param_columns[param] = []
                param_columns[param].append(value)
        
        scores = -results_df['mean_test_score']  # Convertir de negativo
        
        # Analizar cada par√°metro
        for param_name, param_values in param_columns.items():
            unique_values = list(set(param_values))
            if len(unique_values) > 1:  # Solo analizar si hay variaci√≥n
                print(f"\n   üìä {param_name}:")
                
                param_scores = {}
                for value in unique_values:
                    mask = [v == value for v in param_values]
                    value_scores = scores[mask]
                    if len(value_scores) > 0:
                        param_scores[value] = {
                            'mean': value_scores.mean(),
                            'std': value_scores.std(),
                            'count': len(value_scores)
                        }
                
                # Ordenar por score promedio
                sorted_params = sorted(param_scores.items(), key=lambda x: x[1]['mean'])
                
                for value, stats in sorted_params:
                    print(f"      {value}: {stats['mean']:.4f} (¬±{stats['std']:.4f}) [{stats['count']} pruebas]")
                
                # Mostrar el mejor valor para este par√°metro
                best_value = sorted_params[0][0]
                worst_value = sorted_params[-1][0]
                improvement = sorted_params[-1][1]['mean'] - sorted_params[0][1]['mean']
                print(f"      ‚úÖ Mejor: {best_value} | ‚ùå Peor: {worst_value} | üìà Diferencia: {improvement:.4f}")

    def _calculate_overfitting_score(self, train_mse, test_mse):
        if train_mse == 0:
            return float('inf') if test_mse > 0 else 0
        return test_mse / train_mse

    def _show_model_results(self, name, metrics):
        if 'error' in metrics:
            print(f"\n‚ùå {name}: Error - {metrics['error']}")
            return

        print(f"\nüìä RESULTADOS DETALLADOS - {name}")
        print("-" * 40)
        print(f"üîÑ Cross-Validation MSE: {metrics['cv_mse_mean']:.4f} ¬± {metrics['cv_mse_std']:.4f}")
        print(f"üèãÔ∏è  Train MSE: {metrics['train_mse']:.4f}")
        print(f"üéØ Test MSE:  {metrics['test_mse']:.4f}")
        print(f"üìà Test R¬≤:   {metrics['test_r2']:.4f}")

        overfitting = metrics['overfitting_score']
        if overfitting < 1.1:
            print(f"‚úÖ Overfitting Score: {overfitting:.2f} (Bueno)")
        elif overfitting < 1.5:
            print(f"‚ö†Ô∏è  Overfitting Score: {overfitting:.2f} (Moderado)")
        else:
            print(f"üö® Overfitting Score: {overfitting:.2f} (Alto)")

        if metrics['best_params'] != "default":
            print(f"üîß Mejores par√°metros: {metrics['best_params']}")

    def _show_detailed_comparison(self):
        print(f"\n{'='*80}")
        print("üèÜ COMPARACI√ìN DETALLADA DE MODELOS CON CROSS-VALIDATION")
        print(f"{'='*80}")

        print(f"{'Modelo':<15} {'CV MSE':<12} {'Test MSE':<10} {'Test R¬≤':<8} {'Overfitting':<12} {'Estado'}")
        print("-" * 80)

        best_cv_score = float('inf')
        best_model = None

        for name, metrics in self.results.items():
            if 'error' in metrics:
                print(f"{name:<15} {'ERROR':<12} {'ERROR':<10} {'ERROR':<8} {'ERROR':<12} {'‚ùå'}")
                continue

            cv_score = metrics['cv_mse_mean']
            test_mse = metrics['test_mse']
            test_r2 = metrics['test_r2']
            overfitting = metrics['overfitting_score']

            if overfitting < 1.1:
                status = "‚úÖ Bueno"
            elif overfitting < 1.5:
                status = "‚ö†Ô∏è  Moderado"
            else:
                status = "üö® Overfitting"

            print(f"{name:<15} {cv_score:<12.4f} {test_mse:<10.4f} "
                  f"{test_r2:<8.4f} {overfitting:<12.2f} {status}")

            if cv_score < best_cv_score and overfitting < 1.3:
                best_cv_score = cv_score
                best_model = name

        if best_model:
            print(f"\nüèÜ MEJOR MODELO: {best_model}")
            print(f"   üìä CV MSE: {best_cv_score:.4f}")
            print(f"   üéØ Sin overfitting significativo")
        else:
            print(f"\n‚ö†Ô∏è  ADVERTENCIA: Todos los modelos muestran problemas de overfitting")
            print(f"   üí° Considera reducir la complejidad o conseguir m√°s datos")

    def _save_model(self, name, model):
        model_path = f"app/models_cache/{name.lower()}_model.pkl"
        try:
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            print(f"üíæ Modelo optimizado guardado: {model_path}")
        except Exception as e:
            print(f"‚ùå Error guardando modelo: {e}")

    def _save_metadata(self, label_encoder, feature_names):
        try:
            with open("app/models_cache/label_encoder.pkl", 'wb') as f:
                pickle.dump(label_encoder, f)
            with open("app/models_cache/feature_names.pkl", 'wb') as f:
                pickle.dump(feature_names, f)
            print("‚úÖ Metadata guardada")
        except Exception as e:
            print(f"‚ùå Error guardando metadata: {e}")

    def _save_training_results(self, results):
        """Guarda m√©tricas de entrenamiento para selecci√≥n posterior"""
        try:
            results_file = "app/models_cache/training_results.pkl"
            with open(results_file, 'wb') as f:
                pickle.dump(results, f)
            
            print(f"üíæ M√©tricas de entrenamiento guardadas: {results_file}")
            
            # Tambi√©n guardar un resumen legible
            self._save_readable_summary(results)
            
        except Exception as e:
            print(f"‚ùå Error guardando m√©tricas: {e}")

    def _save_readable_summary(self, results):
        """Guarda un resumen legible de los resultados"""
        try:
            summary_file = "app/models_cache/model_comparison.txt"
            
            with open(summary_file, 'w') as f:
                f.write("COMPARACI√ìN DE MODELOS - M√âTRICAS DE ENTRENAMIENTO\n")
                f.write("="*60 + "\n\n")
                
                for name, metrics in results.items():
                    if 'error' in metrics:
                        f.write(f"{name}: ERROR - {metrics['error']}\n\n")
                        continue
                    
                    f.write(f"{name}:\n")
                    f.write(f"  CV MSE: {metrics.get('cv_mse_mean', 'N/A'):.4f}\n")
                    f.write(f"  Test MSE: {metrics.get('test_mse', 'N/A'):.4f}\n")
                    f.write(f"  Test R¬≤: {metrics.get('test_r2', 'N/A'):.4f}\n")
                    f.write(f"  Overfitting Score: {metrics.get('overfitting_score', 'N/A'):.2f}\n")
                    f.write(f"  Mejor Par√°metros: {metrics.get('best_params', 'N/A')}\n\n")
            
            print(f"üìÑ Resumen legible guardado: {summary_file}")
            
        except Exception as e:
            print(f"‚ùå Error guardando resumen: {e}")